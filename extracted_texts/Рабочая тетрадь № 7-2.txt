
--- Страница 1 ---
Дисциплина « Системы искусственного интеллекта и большие данные » 
Рабочая тетрадь № 7 (часть 2)  
Теоретический материал – TensorF low, Keras  и среда выполнения  
GPU  на Colaboratory  
Нейронные сети являются мощным инструментом в области машинного 
обучения и искусственного интеллекта. Они имитируют работу человеческого 
мозга, используя искусственные нейроны для обработки и анализа данных. 
Нейронные сети способны решать широкий спектр за дач, начиная от 
классификации изображений и заканчивая прогнозированием временных 
рядов. В этой части  мы рассмотрим несколько примеров программ нейронных 
сетей, которые помогут вам лучше понять, как они работают и как их можно 
использовать в различных задачах. Мы начнем с простых примеров и 
постепенно перейдем к более сложным моделям.  
TensorFlow  — бесплат ная платформа машинного обучения на Python с 
открытым исходным кодом, разработанная в основном в Google. Как и у 
NumPy, основная цель TensorFlow – дать инженерам и исследователям 
возможность манипулировать математическими выражениями с числовыми 
тензорами.  Но TensorFlow может намного больше, чем NumPy, в том числе:  
 автоматически вычислять градиент любого дифференцируемог о 
выражения, что делает ее прекрасной основой для машинного 
обучения;  
 работать не только на обычных, но также на графических и 
тензорных пр оцессорах — высокопараллельных аппаратных 
ускорителях;  
 распределять вычисления между множеством компьютеров;  
 экспортировать вычисления другим окружениям выполнения, 
таким как C++, JavaScript (для веб -приложений, выполняющихся в 
браузере) или TensorFlow Lite (для приложений, действующих в 
мобильных или встраиваемых устройствах) и т. Д. Это упрощает 
развертывание п риложений TensorFlow в практических условиях.  
Keras  — это высокоуровневая библиотека для создания и обучения 
нейронных сетей на языке Python. Она упрощает процесс разработки моделей, 
предоставляя удобный интерфейс для работы с TensorFlow и другими 
фреймвор ками. Keras позволяет быстро прототипировать модели и легко 
экспериментировать с различными архитектурами нейронных сетей.  
Благодаря TensorFlow библиотека Keras может работать на раз личных 
типах оборудования  – графическом, тензорном или обычном процессоре и 
поддерживает простую возможность распределения вычислений между 
тысячами компьютеров.  (рис. 1).  

--- Страница 2 ---
 
Рис. 1. TensorFlow — это низкоуровневая платформа тензорных 
вычислений, а Keras — высокоуровневая библиотека глубокого обучения  
Colaboratory  и работа со средой выполнения GPU . Colaboratory (или 
просто Colab) — это бесплатная облачная служба для блокнотов Jupyter, не 
требующая установки дополнительного программного обеспечения. По сути, 
это веб -страница, позволяющая сразу же писать и выполнять сценарии, 
использующие Keras. Она дает доступ к бесплатной (но ограниченной) среде 
выполнения на графическом процессоре и даже к среде выполнения на 
тензорном процессоре (TPU), благодаря чему вам не придется покупать свой 
GPU.  Чтобы начать работу со средой  выполнения GPU в Colab , выберите в 
меню пункт Runtime Change Runtime Type (Среда выполнения Сменить 
среду выполнения) и в раскрывающемся списке Hardware Accelerator 
(Аппаратный ус коритель) выберите T4 GPU (рис. 2 ). 
 
Рис. 2. Выбор среды выполнения GPU в C olab 
Для выполнения примеров и заданий данной рабочей тетради 
рекомендуем использовать  Jupyter  Notebook , а также можно  Colaboratory 
(https://colab.research.google.com/ )  со средой выполнения GPU для увеличения 
скорости обучения . 


--- Страница 3 ---
Пример 1. Простой пример нейронной сети на Python с использованием 
библиотеки Keras  
Установка Keras и TensorFlow  
Для начала установим необходимые библиотеки:  
 
Создание простой нейронной сети  
Рассмотрим пример создания простой нейронной сети для задачи 
классификации. В этом примере мы создадим модель, которая будет 
классифицировать данные на два класса. Мы будем использовать 20 входных 
признаков и один выходной нейрон с сигмоидной активацией.  
 
Результат выполнения:  


--- Страница 4 ---
 
Этот пример демонстрирует, как создать и обучить простую нейронную сеть 
для бинарной классификации. Мы используем 20 входных признаков и один 
выходной нейрон с сигмоидной активацией. Модель состоит из двух слоев: 
первый слой содержит 64 нейрона с активацие й ReLU , а второй слой - один 
нейрон с активацией сигмоид. Мы используем функцию потерь 
binary_crossentropy  и оптимизатор adam . 
Задание 1.  
1. Изучите и выполните пример 1 на  Jupyter  Notebook  или Colab .. 
2. Поэкспериментируйте с новыми параметрами :   
 Перый слой содержит 128 нейронов с активацией tanh 
 Второй слой – один  нейрон с активацией softmax  
 Изменить оптимизатор на rmsprop.  
Решение  
 
 
Пример 2.  Пример нейронной сети для классификации изображений с 
использованием TensorFlow  
Перед нами стоит задача: реализовать классификацию черно -белых 
изображений рукописных цифр (28 × 28 пикселей) по десяти категориям (от 0 
до 9). Мы будем использовать набор данных MNIST, популярный в сообществе 
исследователей глубокого обучения, который существует практичес ки столько 
же, сколько сама область машинного обучения, и широко используется для 
обучения. Этот набор содержит 60000 обучающих изображений и 10 000 
контрольных изображений, собранных Национальным институтом стандартов 


--- Страница 5 ---
и технологий США (National Institute o f Standards and Technology — часть NIST 
в аббревиатуре MNIST) в 1980 -х годах. «Решение» задачи MNIST можно 
рассматривать как своеобразный аналог Hello World в глубоком обучении — 
часто это первое действие, которое выполняется для уверенности, что 
алгоритмы  действуют в точности как ожидалось. По мере углубления в 
практику машинного обучения вы увидите, что MNIST часто упоминается в 
научных статьях, блогах и т. д. Некоторые образцы изображений из набор а 
MNIST можно видеть на рис. 3 . 
 
Рис. 3 . Образцы изображе ний MNIST  
Набор данных MNIST уже входит в состав Keras в форме набора из 
четырех массивов NumPy.  
Загрузка набора данных MNIST в Keras  
 
Здесь train_images  и train_labels  — это обучающий набор , то есть данные, на 
которых модель обучается. После обучения модель будет проверяться 
тестовым (или контрольным) набором, test_images и test_labels. Здесь 
train_images и train_labels — это обучающий набор , то есть данные, на которых 
модель обучается. Посл е обучения модель будет проверяться тестовым (или 
контрольным) набором, test_images и test_labels.  
Изображения хранятся в массивах NumPy, а метки — в массиве цифр от 
0 до 9. Изображения и метки находятся в прямом соответствии, один к одному.  
Рассмотрим обучающие данные:  
 
 
 
 
 
 
И контрольные данные:  
 
 
 
 
  
 


--- Страница 6 ---
 
Вот как мы будем действовать дальше: сначала передадим нейронной 
сети обучающие данные, train_images и train_labels. Сеть обучится подбирать 
правильные метки для изображений. А затем мы предложим ей 
классифицировать изображения в test_images и проверим точность 
классификации по меткам из test_labels.  
Теперь сконструируем сеть . 
 
Основным строительным блоком нейронных сетей является слой . Слой 
можно рассматривать как фильтр для данных: он принимает их и выводит в 
некоторой более полезной форме. В частности, слои извлекают представления  
из входных данных, которые, как мы надеемся, будут иметь больше смысла для 
решаемой задачи. Фактически методика глубокого обучения заключается в 
объединении простых слоев, реализующих некоторую форму поэтапной 
очистки  данных.  
Модель глубокого обучения можно сравнить с ситом, состоящим из 
последова тельности фильтров — слоев — все более тонкой работы с данными.  
В нашем случае сеть состоит из последовательности двух слоев Dense, 
которые являются тесно связанными (их еще называют полносвязными ) 
нейронными слоями. Второй (и последний) слой — это десятипеременный слой 
классифика ции softmax , возвращающий массив с десятью оценками 
вероятностей (в сумме дающих 1). Каждая оценка определяет вероятность 
принадлежности текущего изображения к одному из десяти классов цифр.  
Чтобы подготовить модель к обучению, нужно настроить еще три 
параметра для этапа компиляции : 
 оптимизатор  — механизм, с помощью которого сеть будет 
обновлять себя, опираясь на наблюдаемые данные и функцию 
потерь;  
 функцию потерь  — определяет, как сеть должна оценивать 
качество своей работы на обучающих данных и, соответственно, 
корректировать ее в пра вильном направлении;  
 метрики для мониторинга на этап ах обучения и тестирования  — 
здесь нас будет интересовать только точность (доля правильно 
классифицированных изображений).  
Этап компиляции : 


--- Страница 7 ---
 
Перед обучением мы выполним предварительную обработку данных, 
преобразовав в форму, которую ожидает получить нейро нная сеть, и 
масштабируем их так, чтобы все значения оказались в интервале [0, 1]. 
Исходные данные — обучающие изображения — хранятся в трехмерном 
массиве (60000, 28, 28) типа uint8, значениями в котором являются числа в 
интервале [0, 255]. Мы преобразуем его в массив (60000, 28 * 28) типа float32 
со значениями в интервале [0, 1].  
Подготовка исходных данных : 
 
Теперь можно начинать обучение сети, для чего в случае библиотеки 
Keras  достаточно вызвать метод fit модели — он попытается адаптировать  (fit) 
модель  под обучающие данные.  
Обучение («адаптация») модели : 
 
 
В процессе обучения отображаются две величины: потери сети на 
обучающих данных и точность сети на обучающих данных. Мы быстро 
достигли точности 0,9894 (98,94 %).  
Теперь у нас есть обученная модель, которую можно использовать для 
прогнозирования вероятностей принадлежности новых цифр к классам — 
изображений, которые не входили в обучающую выборку, как те из 
контрольного набора.  
Использование модели для получения предсказаний:  
 


--- Страница 8 ---
 
Каждое число в элемен те массива с индексом i соответствует вероятности 
принадлежности изображения цифры test_digits[0] к классу i.  
Наивысшая оценка вероятности (0,999 4940  — почти 1) для этого 
тестового изображения цифры находится в элементе с индексом 7, то есть 
согласно нашей модели — перед нами изображение цифры 7:  
 
 
 
 
Прогноз можно проверить по массиву меток:  
 
 
В целом, насколько хорошо справляется наша модель с классификацией 
прежде не встречавшихся ей цифр? Давайте проверим, вычислив среднюю 
точность по всему контрольному набору изображений.  
Оценка качества модели на новых данных:  
 
 
Точность на контрольном наборе составила 97,8 8 % — немного меньше, 
чем на обуч ающем (98,9 4 %). Эта разница демонстрирует пример переобучения 
(overfitting), когда модели машинного обучения показывают точность на новом 
наборе данных худшую, чем на обучающем.  
Задание 2.  
Изучите и выполните пример 2  на Jupyter  Notebook  или Colab . Измените 
параметры:  
 размер мини -выборки (batch_size)  на 64  
 число эпох  (epoch s) на 20  
 Посмотрите новый результат точности на контрольном наборе и 
сравните с результатом примера 2.  
Решение  
 
 
  


--- Страница 9 ---
Пример 3. Пример простой с верточной нейронной сети  
Рассмотрим  практический пример простой сверточной нейронной сети, 
классифицирующей изображения рукописных цифр из набора MNIST.  Эту 
задачу мы решили в примере  2, использовав полносвязную сеть (ее точность на 
контрольных данных составила 97,8 8 %). Несмотря на простот у сверточной 
нейронной сети, ее точность будет значительно выше полносвязной модели из 
примера  2. 
В следующем листинге показано, как выглядит простая сверточная 
нейронная сеть. Это стек слоев Conv2D  и MaxPooling2D . Как она действует, 
рассказывается чуть ни же. Мы построим модел ь с помощью функционального 
API. 
Создание небольшой сверточной нейронной сети : 
 
Важно отметить, что данная сеть принимает на входе тензоры с формой 
(высота_изображения, ширина_изображения, каналы), не включая измерение, 
определяющее п акеты. В данном случае мы настроили сеть на обработку входов 
с размерами (28, 28, 1), соответствующими формату изображений в наборе 
MNIST.  
Рассмотрим поближе текущую архитектуру сети.  
Сводная информация о сети : 
 


--- Страница 10 ---
 
Как видите, все слои Conv2D  и MaxPooling2D  выводят трехмерный тензор 
с формой (высота, ширина, каналы). Измерения ширины и высоты сжимаются с 
ростом  глубины сети. Количество каналов управляется первым аргументом, 
передаваемым в слои Conv2D  (32, 64 или 128).  
Последний слой Conv2D  выдает  результат с формой (3, 3, 128) — карту 
признаков 3 × 3 со 128 каналами. Следующий шаг — передача этого результата 
на вход полносвязной классифицирующей сети, подобной той, с которой мы уже 
знакомы: стека слоев Dense . Эти классификаторы обрабатывают вектор ы — 
одномерные массивы, — тогда как текущий выход является трехмерным 
тензором. Чтобы преодолеть это несоответствие, мы преобразуем трехмерный 
вывод в одномерный с помощью слоя Flatten , а затем добавляем полносвязные 
слои Dense . 
В заключение выполняется кл ассификация по десяти категориям, поэтому 
последний слой имеет десять выходов и активацию softmax.  
Теперь обучим сверточную сеть распознаванию цифр MNIST. Мы будем 
повторно брать большое количество программного кода из примера  2. 
Поскольку модель выполняет  классификацию по десяти категориям с 
активацией softmax , мы используем функцию потерь категориальной 
перекрестной энтропии, а так как метки являются целыми числами, нам 
понадобится разреженная версия sparse_categorical_crossentropy . 
Обучение сверточной не йронной сети на данных из набора MNIST : 


--- Страница 11 ---
 
Оценим модель на контрольных данных.  Оценка сверточной сети:  
 
 
Полносвязная сеть из примера  2 показала точность 97,8 8 % на 
контрольных данных, а простенькая с верточная нейронная сеть — 99,1 %: мы 
уменьшили процент ошибок на 68 % (относительно). Неплохо!  
Задание 3.  
1. Изучите и выполните пример 3 на Jupyter Notebook или Colab.  
2. На основе блокнота (ноутбука) примера 3 постройте новую модель 
сверточных нейронных сетей с 5 слоями Conv 2D и 4 слоями 
MaxPooling2D  для классификации изображений на наборе данных 
MNIST . 
3. Обучите модель и сравните новый результат точности с результатом в 
примере 3.  
Решение  
 
 
Пример 4. Пример сверточной нейронной сети для распознавания 
объектов  на изображениях  из набора данных CIFAR -10 
Сверточные нейронные сети ( Convolutional  Neural  Networks  - CNN) 
особенно эффективны для обработки изображений. Они используют 
сверточные слои для автоматического извлечения признаков из изображений. 
Рассмотрим пример использования CNN для распознавания объектов на 
изображениях из набора данных CIFAR -10. 
CIFAR -10 - это один из самых популярных наборов данных для обучения 
моделей компьютерного зрения. Он содержит 60,000 цветных изображений 
размером 32x32 пикселя, разделенных на 10 классов (самолёт, автомобиль, 
птица, кот, олень, собака, лягушка, лошадь , корабль и грузовик ), по 6000 
изображений на класс. Имеется 50000 обучающих изображений и 10000 


--- Страница 12 ---
тестовых изображений.  Вот классы в наборе данных, а также 10 случайных 
изображений из каждого  (рис. 4) : 
 
Рис. 4. Набор данных CIFAR -10 
Набор данных разделен на пять обучающих пакетов и один тестовый 
пакет, каждый из которых содержит 10000 изображений. Тестовый пакет 
содержит 1000 случайно выбранных изображений из каждого класса. 
Обучающие пакеты со держат оставшиеся изображения в случайном порядке, 
но некоторые обучающие пакеты могут содержать больше изображений из 
одного класса, чем из другого. Между ними обучающие пакеты содержат 5000 
изображений из каждого класса.  
Для распознавания объектов на изо бражениях из набора данных CIFAR -
10 мы будем использовать такую сверточную нейронную сеть:  
 
Сеть включает два каскада из слоев свертки и подвыборки (всего 6 
слоев) , которые предназначены для выделения признаков изображений . Затем 
следует классификатор из двух полносвязных слоев  (512 и 10 нейронов) . 
Также как и при распознавании цифр выходной слой содержит вероятности 
принадлежности изображения к тому или иному классу.  
На вход нейр онной сети поступают изображения размером 32 x32 в трех 
каналах ( RGB  - красный, зеленный и синий) . На первом слое свёртки 
используются 32 карты признаков размера 3 x3, т.е. каждый нейрон 


--- Страница 13 ---
сверточного слоя подкючен к квадратному участку. Всего на этом слое 
используются 32 разных карт признаков.  Следующий свёрточный слой имеет 
такую же архитектуру 32 карты признаков с ядром свёртки 3 x3. После этого 
идёт слой подвыборки, на который выполняется уменьшение размерности  для 
каждой карты признаков  отдельно, поэтому здесь тоже используются 32 карты 
и размер поля подвыборки 2x2.  
После сл оя подвыборки начинается новый каскад сверточных слоев. На 
третьем и на четвертом слое свертки используются 64 карты признаков 
размером 3x3. А на втором слое подвыборки , которые следуют после этих 
сверточных слоев также происходит уменьшение размерности в квадрате 2 x2. 
После этого данные преобразуются из двумерного формата в одномерный и 
передаются на полносвязный слой на котором уже и выполняется 
классификация.  
Теперь рассмотрим как реализовать такую сеть с помощью библиотеки 
Keras  и обучить её на наборе данных CIFAR -10. Как всегда сначала выполняем 
импорт необходимых элементов из библиотеки Keras  и Numpy : 
 
Задаем seed для повторяемости результатов : 
 
Загружаем данные  из набора CIFAR -10: 
 
Список с названиями классов набора данных CIFAR -10: 
 
Просматриваем примеры изображений : 


--- Страница 14 ---
 
Мы получим примеры изображений:  
 
Установим параметры:  
 


--- Страница 15 ---
Нам необходимо выполнить предварительную обработку данных. 
Нормализуем данные о интенсивности  пикселей изображений , чтобы все они 
находились в диапазоне [0,1] . Для этого преобразуем их в тип float32 и делим 
на 255:  
 
А метки классов необходимо преобразовать в категории:  
 
Наша сеть имеет 10 нейронов и  выходной сигнал нейронов 
соответствует вероятности того, что изобра жение принадлежит к данному 
классу. Соответственно, номера классов в метках мы должны преобразовать в 
представление по категориям.  
Теперь, когда наши данные подготовлены мы можем приступить к 
созданию сети. Создаем модель  Sequential  – последовательная сеть, где слои 
идут друг за другом, и перый каскад из слоев свертки и подвыборки.  
 
Добавляем в модель свёрточный слой, который работает с двухмерным и 
данным и тип Conv 2D: 
 
Этот слой будет иметь 32 карты признаков, размер ядра свертки на 
каждой карт е 3x3. Размер входных данных 32x32x3, что соответствует трём 
каналам изображений для кодов трёх цветов RGB  размера изображения 32 x32. 
В качестве функции активации используем ReLU . 
Второй сверточный слой устроен точно так же 32 карты признаков 
размером 3x3: 
 
Затем идет слой подвыборки , размер уменьшения размерности 2 x2 и в 
качестве слоя подвыборки мы используем  MaxPooling 2D, т.е. из квадратика 
размером 2x2 выбирается максимальное значение.  
  


--- Страница 16 ---
После каскада из двух сверточных слоев и слоя подвыборки мы 
добавляе м слой регуляризации Dropout : 
 
Dropout  – одна из техник  предотвращения переобучения (overfitting ). В 
сверточных нейронных сетях, переобучение часто возникает когда 
находящиеся друг с другом нейроны настраиваются на совместную работу. За 
счёт этого они настраиваются на особенности конкретной выборки, а не на 
общие закономерности, характерные для различных изображений.  Техника 
Dropout  позволяет достаточно просто и эффективно снизить переобучение, 
которое возникает в сверточные нейронные сети. Для этого в процессе 
обучения когда на вход нейронной сети подается каждый новый объект  
случайным образом выключаются некоторое количество нейронов с заданной 
вероятностью . Dropout  0.25 означает, что нейрон будет отключаться с 
вероятностью 25%. Оставшие ся нейроны обу чаются распознавать 
необходимые признаки без участия соседних нейронов.  
После слоя регуляризации идёт ещё один каскад из двух сверточных 
слоев и слоя подвыборки. Но в этом каскаде больше карт признаков – 64. 
 
Слой подвыборки устроен точно также: выбор максимального значения 
из квадратика размером 2 x2. После слоя подвыборки снова идет слой 
регуляризации Dropout,  который выключает нейроны с вероятностью 25%.  
После двух каскадов сверточных слоев и слоев подвыборки следует 
классификатор, который по признака м найденной сверточной сетью 
выполняет определение к какому конкретно классу принадлежит объект на 
картинке. Сначала  нам необходимо преобразовать нашу сеть из двумерного 
представления в плоское. Для этого добавляем слой Flatten  и затем добавляем 
два полнос вязных слоев типа Dense . В слое  содержится 512 нейронов , 
используется функция активации ReLU . В выходном слое содержится 10 
нейронов по количеству  классов. На этом слое используется функция 
активации softmax , которая соответствует вероятности появления тог о или 
иного класса.  


--- Страница 17 ---
 
Суммарное выходное значение всех 10 нейронов рав но единице.  Между 
двумя полносвязными слоями у нас есть слой регуляризации  Dropout , который 
выключает нейроны в этот раз с вероятностью 50%. Сеть, которую мы создали 
в этом примере уже является глубокой. В ней используются 8 слоев (4 
сверточных слоев , 2 слоя подвыборки и 2 полносвязных слоев.  
После того как мы задали сеть, можно её скомпилировать . Вызываем 
метод model .compile , в качестве функции ошибки используем 
categorical_crossentropy , которое хорошо подходит когда на выходе у нас 
значение вероятности появления классов. Оптимизируем с помощью 
стохастического метода градиентного сп уска и в качестве метрики используем 
точность accuracy : 
 
Вызываем метод fit для обучения сети. Обуч аем сеть на данных для 
обучения  X_train  содержит изображение для обучения Y_train  правильные 
ответы уже в преобразованные в предоставление по категориям. Параметр 
validation_split  говорит о том, что мы разбиваем набора  X_train  и Y_train  на две 
части: обучающая выборка 90% и проверочная выборка 10% . Разбивку и 
проверку качества обучения на обеих выборках  Keras  выполняет 
автоматически . Размер мини -выборки (batch _size = 32) , т.е. мы изменяем веса 
нейронных сетей после того как обрабатываем каждые 32 объекта и обучение 
сети выполняется в течение 25 эпох (nb_epoch ). Параметр shuffle  
установленный в True , это значение по умолчанию говорит о том, что 
библиотека Keras  в начале каждой эпохи будет перемешивать данные, чтобы 
они шли в разном порядке. Это повышает качество обучения, т.к. мы 
используем  стохастический метод градиентн ого спуска . 


--- Страница 18 ---
 
 
------------  
 
Оцениваем качество обучения модели на тестовых данных : 
 
 
Теперь мы сгенерируем графики с помощью  библиотеки matplotlib  для 
визуализации потерь при обучении и проверке, а также изменения точности по 
эпохам, используя модель history : 
 


--- Страница 19 ---
 
 
 
Задание 3.  
1. Изучите и выполните пример 4 на Jupyter  Notebook  или Colab .  
2. Попробуйте изменить нейронную сеть, чтобы улучшить качество 
решения:  
 Изменяйте количество нейронов в слоях  
 Добавляйте новые скрытые слои  
 Изменяйте количество эпох обучения  
 Изменяйте размер мини -выборки (batch_size)  
3. После подбора лучших гиперпараметров, обучите сеть еще раз на 
полном объеме данных без разделения на обучающий и проверочный 
наборы. Во время обучения следите, чтобы не возникло переобучение.  
Решение  
 
 
