{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –†–¢ 7: –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏\n",
        "\n",
        "–ò–∑—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π: –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω, TensorFlow, Keras\n",
        "\n",
        "**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:**\n",
        "\n",
        "- –ß–∞—Å—Ç—å 1: –û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
        "- –ß–∞—Å—Ç—å 2: TensorFlow –∏ Keras\n",
        "- –ü—Ä–∏–º–µ—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "- –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN)\n",
        "\n",
        "–¢–û –ß–¢–û –í–ù–ò–ó–£ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ó–ê–ü–£–°–¢–ò–¢–¨ –ï–°–õ–ò –ù–ï –í –ö–û–õ–ê–ë–ï –û–¢–ö–†–´–¢–û\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -q numpy pandas matplotlib scikit-learn scipy seaborn tensorflow keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ß–∞—Å—Ç—å 1: –û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
        "\n",
        "–ü–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω - —ç—Ç–æ —ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–∞—è —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –ª–∏–Ω–µ–π–Ω—ã–π –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.\n",
        "\n",
        "### –ü—Ä–∏–º–µ—Ä - –ö–ª–∞—Å—Å –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –Ω–∞ Python\n",
        "\n",
        "–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç —É—á–∏—Ç—å—Å—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    –ö–ª–∞—Å—Å –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "    \"\"\"\n",
        "    def __init__(self, n_inputs, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        n_inputs - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–æ–≤\n",
        "        learning_rate - —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
        "        \"\"\"\n",
        "        self.weights = np.zeros(n_inputs + 1)  # +1 –¥–ª—è –ø–æ—Ä–æ–≥–∞ w0\n",
        "        self.learning_rate = learning_rate\n",
        "    \n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
        "        \"\"\"\n",
        "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
        "        return 1 if summation >= 0 else -1\n",
        "    \n",
        "    def train(self, training_inputs, labels, epochs=10):\n",
        "        \"\"\"\n",
        "        –û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            errors = 0\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "                if prediction * label < 0:\n",
        "                    self.weights[1:] += self.learning_rate * label * inputs\n",
        "                    self.weights[0] += self.learning_rate * label\n",
        "                    errors += 1\n",
        "            print(f\"–≠–ø–æ—Ö–∞ {epoch + 1}: –æ—à–∏–±–æ–∫ = {errors}\")\n",
        "            if errors == 0:\n",
        "                print(\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "                break\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω\n",
        "# –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–∞, –≥–¥–µ –ø–µ—Ä–≤–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –±–æ–ª—å—à–µ –≤—Ç–æ—Ä–æ–π (–∫–ª–∞—Å—Å 1)\n",
        "# –∏ –≥–¥–µ –ø–µ—Ä–≤–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –º–µ–Ω—å—à–µ –≤—Ç–æ—Ä–æ–π (–∫–ª–∞—Å—Å -1)\n",
        "training_data = np.array([\n",
        "    [3, 1],\n",
        "    [4, 2],\n",
        "    [1, 3],\n",
        "    [2, 4],\n",
        "    [5, 2],\n",
        "    [1, 5]\n",
        "])\n",
        "\n",
        "labels = np.array([1, 1, -1, -1, 1, -1])\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω\n",
        "perceptron = Perceptron(n_inputs=2, learning_rate=0.1)\n",
        "print(\"–ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞:\", perceptron.weights)\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞:\")\n",
        "perceptron.train(training_data, labels, epochs=10)\n",
        "print(\"\\n–ò—Ç–æ–≥–æ–≤—ã–µ –≤–µ—Å–∞:\", perceptron.weights)\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞:\")\n",
        "print(\"=\"*60)\n",
        "test_data = [[6, 2], [2, 6], [3, 3], [5, 1]]\n",
        "for test in test_data:\n",
        "    prediction = perceptron.predict(test)\n",
        "    print(f\"–í—Ö–æ–¥ {test} ‚Üí –ö–ª–∞—Å—Å {prediction}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞–Ω–∏–µ 1 - –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞ 1:** –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å —Ç—Ä–µ–º—è –≤—Ö–æ–¥–∞–º–∏ (x1, x2, x3), —Ç—Ä–µ–º—è –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ (h1, h2, h3) –∏ –æ–¥–Ω–∏–º –≤—ã—Ö–æ–¥–æ–º (o1).\n",
        "\n",
        "- –í–µ—Å–∞: w = [0.5, 0.5, 0.5]\n",
        "- –ü–æ—Ä–æ–≥: b = 0\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞ 2:** –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å –¥–≤—É–º—è –≤—Ö–æ–¥–∞–º–∏ (x1, x2), –¥–≤—É–º—è –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–ª–æ–µ (h1, h2) –∏ –¥–≤—É–º—è –≤—ã—Ö–æ–¥–∞–º–∏ (o1, o2).\n",
        "\n",
        "- –í–µ—Å–∞: w = [1, 0]\n",
        "- –ü–æ—Ä–æ–≥: b = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# –ó–∞–¥–∞—á–∞ 1: –¢—Ä–∏ –≤—Ö–æ–¥–∞, —Ç—Ä–∏ —Å–∫—Ä—ã—Ç—ã—Ö –Ω–µ–π—Ä–æ–Ω–∞, –æ–¥–∏–Ω –≤—ã—Ö–æ–¥\n",
        "class NeuralNetwork3Input:\n",
        "    def __init__(self):\n",
        "        self.w = np.array([0.5, 0.5, 0.5])\n",
        "        self.b = 0\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        \"\"\"\n",
        "        x - –≤–µ–∫—Ç–æ—Ä –∏–∑ —Ç—Ä–µ—Ö –≤—Ö–æ–¥–æ–≤ [x1, x2, x3]\n",
        "        \"\"\"\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π: 3 –Ω–µ–π—Ä–æ–Ω–∞\n",
        "        h1 = sigmoid(np.dot(self.w, x) + self.b)\n",
        "        h2 = sigmoid(np.dot(self.w, x) + self.b)\n",
        "        h3 = sigmoid(np.dot(self.w, x) + self.b)\n",
        "        \n",
        "        h = np.array([h1, h2, h3])\n",
        "        \n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        o1 = sigmoid(np.dot(self.w, h) + self.b)\n",
        "        \n",
        "        return o1\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º\n",
        "network1 = NeuralNetwork3Input()\n",
        "test_input1 = np.array([1, 2, 3])\n",
        "output1 = network1.feedforward(test_input1)\n",
        "print(\"–ó–∞–¥–∞—á–∞ 1: –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å —Ç—Ä–µ–º—è –≤—Ö–æ–¥–∞–º–∏\")\n",
        "print(f\"–í—Ö–æ–¥: {test_input1}\")\n",
        "print(f\"–í—ã—Ö–æ–¥: {output1:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# –ó–∞–¥–∞—á–∞ 2: –î–≤–∞ –≤—Ö–æ–¥–∞, –¥–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö –Ω–µ–π—Ä–æ–Ω–∞, –¥–≤–∞ –≤—ã—Ö–æ–¥–∞\n",
        "class NeuralNetwork2Input2Output:\n",
        "    def __init__(self):\n",
        "        self.w = np.array([1, 0])\n",
        "        self.b = 1\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        \"\"\"\n",
        "        x - –≤–µ–∫—Ç–æ—Ä –∏–∑ –¥–≤—É—Ö –≤—Ö–æ–¥–æ–≤ [x1, x2]\n",
        "        \"\"\"\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π: 2 –Ω–µ–π—Ä–æ–Ω–∞\n",
        "        h1 = sigmoid(np.dot(self.w, x) + self.b)\n",
        "        h2 = sigmoid(np.dot(self.w, x) + self.b)\n",
        "        \n",
        "        h = np.array([h1, h2])\n",
        "        \n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: 2 –≤—ã—Ö–æ–¥–∞\n",
        "        o1 = sigmoid(np.dot(self.w, h) + self.b)\n",
        "        o2 = sigmoid(np.dot(self.w, h) + self.b)\n",
        "        \n",
        "        return np.array([o1, o2])\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º\n",
        "network2 = NeuralNetwork2Input2Output()\n",
        "test_input2 = np.array([2, 3])\n",
        "output2 = network2.feedforward(test_input2)\n",
        "print(\"–ó–∞–¥–∞—á–∞ 2: –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å –¥–≤—É–º—è –≤—Ö–æ–¥–∞–º–∏ –∏ –¥–≤—É–º—è –≤—ã—Ö–æ–¥–∞–º–∏\")\n",
        "print(f\"–í—Ö–æ–¥: {test_input2}\")\n",
        "print(f\"–í—ã—Ö–æ–¥—ã: o1 = {output2[0]:.6f}, o2 = {output2[1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"ReLU —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"–ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"–°–∏–≥–º–æ–∏–¥–∞\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π\n",
        "class NeuralNetworkReLU:\n",
        "    def __init__(self):\n",
        "        self.w1 = np.random.randn(3, 2) * 0.5  # –í–µ—Å–∞ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        self.b1 = np.zeros(3)\n",
        "        self.w2 = np.random.randn(1, 3) * 0.5  # –í–µ—Å–∞ –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
        "        self.b2 = np.zeros(1)\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π —Å ReLU\n",
        "        h = relu(np.dot(x, self.w1.T) + self.b1)\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å —Å–∏–≥–º–æ–∏–¥–æ–π\n",
        "        o = sigmoid(np.dot(h, self.w2.T) + self.b2)\n",
        "        return o\n",
        "\n",
        "# –ù–µ–π—Ä–æ—Å–µ—Ç—å —Å tanh –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π\n",
        "class NeuralNetworkTanh:\n",
        "    def __init__(self):\n",
        "        self.w1 = np.random.randn(3, 2) * 0.5\n",
        "        self.b1 = np.zeros(3)\n",
        "        self.w2 = np.random.randn(1, 3) * 0.5\n",
        "        self.b2 = np.zeros(1)\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π —Å tanh\n",
        "        h = tanh(np.dot(x, self.w1.T) + self.b1)\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π —Å —Å–∏–≥–º–æ–∏–¥–æ–π\n",
        "        o = sigmoid(np.dot(h, self.w2.T) + self.b2)\n",
        "        return o\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\n",
        "test_input = np.array([0.5, 0.8])\n",
        "\n",
        "print(\"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏\")\n",
        "print(\"=\"*60)\n",
        "print(f\"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {test_input}\\n\")\n",
        "\n",
        "net_relu = NeuralNetworkReLU()\n",
        "output_relu = net_relu.feedforward(test_input)\n",
        "print(f\"–ù–µ–π—Ä–æ—Å–µ—Ç—å —Å ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π:\")\n",
        "print(f\"  –í—ã—Ö–æ–¥: {output_relu[0]:.6f}\\n\")\n",
        "\n",
        "net_tanh = NeuralNetworkTanh()\n",
        "output_tanh = net_tanh.feedforward(test_input)\n",
        "print(f\"–ù–µ–π—Ä–æ—Å–µ—Ç—å —Å tanh –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π:\")\n",
        "print(f\"  –í—ã—Ö–æ–¥: {output_tanh[0]:.6f}\\n\")\n",
        "\n",
        "print(\"–§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ß–∞—Å—Ç—å 2: Scikit-Learn - MLPClassifier –∏ MLPRegressor\n",
        "\n",
        "### –ó–∞–¥–∞–Ω–∏–µ - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Scikit-Learn\n",
        "\n",
        "**–ó–∞–¥–∞—á–∞:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLPClassifier –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ò—Ä–∏—Å–æ–≤ –∏ MLPRegressor –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∑–∞—Ä–ø–ª–∞—Ç—ã –æ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "# ========== –ß–ê–°–¢–¨ 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ò—Ä–∏—Å–æ–≤ ==========\n",
        "print(\"=\"*70)\n",
        "print(\"–ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ò–†–ò–°–û–í –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú MLPClassifier\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Iris\n",
        "df_iris = pd.read_csv('data/iris.csv')\n",
        "\n",
        "print(f\"\\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df_iris.shape}\")\n",
        "print(f\"–ö–ª–∞—Å—Å—ã: {df_iris['variety'].unique()}\")\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "X_iris = df_iris.drop('variety', axis=1).values\n",
        "y_iris = df_iris['variety'].values\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "scaler_iris = StandardScaler()\n",
        "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
        "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ MLPClassifier\n",
        "mlp_classifier = MLPClassifier(\n",
        "    hidden_layer_sizes=(10, 10),  # –î–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è –ø–æ 10 –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MLPClassifier...\")\n",
        "mlp_classifier.fit(X_train_iris_scaled, y_train_iris)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "y_pred_iris = mlp_classifier.predict(X_test_iris_scaled)\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "accuracy = accuracy_score(y_test_iris, y_pred_iris)\n",
        "print(f\"\\n–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n–û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\")\n",
        "print(classification_report(y_test_iris, y_pred_iris))\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –º–æ–¥–µ–ª–∏\n",
        "print(f\"\\n–ê—Ç—Ä–∏–±—É—Ç—ã –º–æ–¥–µ–ª–∏:\")\n",
        "print(f\"  loss_ (—Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å): {mlp_classifier.loss_:.6f}\")\n",
        "print(f\"  n_iter_ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π): {mlp_classifier.n_iter_}\")\n",
        "print(f\"  n_layers_ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤): {mlp_classifier.n_layers_}\")\n",
        "print(f\"  out_activation_ (—Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è): {mlp_classifier.out_activation_}\")\n",
        "\n",
        "# ========== –ß–ê–°–¢–¨ 2: –†–µ–≥—Ä–µ—Å—Å–∏—è –∑–∞—Ä–ø–ª–∞—Ç—ã ==========\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"–†–ï–ì–†–ï–°–°–ò–Ø –ó–ê–†–ü–õ–ê–¢–´ –û–¢ –û–ü–´–¢–ê –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú MLPRegressor\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞—Ä–ø–ª–∞—Ç–∞—Ö\n",
        "df_salary = pd.read_csv('data/Salary_Data.csv')\n",
        "\n",
        "print(f\"\\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df_salary.shape}\")\n",
        "print(f\"\\n–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏:\")\n",
        "print(df_salary.head())\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "X_salary = df_salary[['YearsExperience']].values\n",
        "y_salary = df_salary['Salary'].values\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "X_train_salary, X_test_salary, y_train_salary, y_test_salary = train_test_split(\n",
        "    X_salary, y_salary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_salary_scaled = scaler_X.fit_transform(X_train_salary)\n",
        "X_test_salary_scaled = scaler_X.transform(X_test_salary)\n",
        "y_train_salary_scaled = scaler_y.fit_transform(y_train_salary.reshape(-1, 1)).ravel()\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ MLPRegressor\n",
        "mlp_regressor = MLPRegressor(\n",
        "    hidden_layer_sizes=(20, 20),  # –î–≤–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è –ø–æ 20 –Ω–µ–π—Ä–æ–Ω–æ–≤\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ MLPRegressor...\")\n",
        "mlp_regressor.fit(X_train_salary_scaled, y_train_salary_scaled)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "y_pred_salary_scaled = mlp_regressor.predict(X_test_salary_scaled)\n",
        "y_pred_salary = scaler_y.inverse_transform(y_pred_salary_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "r2 = r2_score(y_test_salary, y_pred_salary)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_salary, y_pred_salary))\n",
        "\n",
        "print(f\"\\nR¬≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏): {r2:.4f}\")\n",
        "print(f\"RMSE (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞): ${rmse:.2f}\")\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –º–æ–¥–µ–ª–∏\n",
        "print(f\"\\n–ê—Ç—Ä–∏–±—É—Ç—ã –º–æ–¥–µ–ª–∏:\")\n",
        "print(f\"  loss_ (—Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å): {mlp_regressor.loss_:.6f}\")\n",
        "print(f\"  n_iter_ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π): {mlp_regressor.n_iter_}\")\n",
        "print(f\"  n_layers_ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤): {mlp_regressor.n_layers_}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
        "axes[0].scatter(X_test_salary, y_test_salary, color='blue', s=100, \n",
        "                alpha=0.6, edgecolors='black', label='–†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ')\n",
        "axes[0].scatter(X_test_salary, y_pred_salary, color='red', s=100, \n",
        "                alpha=0.6, edgecolors='black', label='–ü—Ä–æ–≥–Ω–æ–∑')\n",
        "axes[0].set_xlabel('–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã (–≥–æ–¥—ã)', fontsize=12)\n",
        "axes[0].set_ylabel('–ó–∞—Ä–ø–ª–∞—Ç–∞ ($)', fontsize=12)\n",
        "axes[0].set_title(f'MLPRegressor: –ü—Ä–æ–≥–Ω–æ–∑ –∑–∞—Ä–ø–ª–∞—Ç—ã (R¬≤ = {r2:.4f})', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ —Ä–µ–∞–ª—å–Ω—ã–µ vs –ø—Ä–æ–≥–Ω–æ–∑\n",
        "axes[1].scatter(y_test_salary, y_pred_salary, alpha=0.6, edgecolors='black')\n",
        "axes[1].plot([y_test_salary.min(), y_test_salary.max()], \n",
        "             [y_test_salary.min(), y_test_salary.max()], \n",
        "             'r--', lw=3, label='–ò–¥–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑')\n",
        "axes[1].set_xlabel('–†–µ–∞–ª—å–Ω–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ ($)', fontsize=12)\n",
        "axes[1].set_ylabel('–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ ($)', fontsize=12)\n",
        "axes[1].set_title('–†–µ–∞–ª—å–Ω—ã–µ vs –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n–ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "np.random.seed(42)\n",
        "X_train = np.random.random((1000, 20))  # 1000 –ø—Ä–∏–º–µ—Ä–æ–≤, 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "y_train = np.random.randint(2, size=(1000, 1))  # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏\n",
        "\n",
        "X_test = np.random.random((200, 20))\n",
        "y_test = np.random.randint(2, size=(200, 1))\n",
        "\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='tanh', input_shape=(20,)),  # 128 –Ω–µ–π—Ä–æ–Ω–æ–≤, tanh\n",
        "    layers.Dense(1, activation='sigmoid')  # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º sigmoid, –∞ –Ω–µ softmax\n",
        "])\n",
        "\n",
        "# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º rmsprop\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# –í—ã–≤–æ–¥ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏\n",
        "print(\"\\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:\")\n",
        "model.summary()\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"\\n–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
        "ax1.plot(history.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax1.plot(history.history['val_accuracy'], label='–ü—Ä–æ–≤–µ—Ä–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax1.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')\n",
        "ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å\n",
        "ax2.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax2.plot(history.history['val_loss'], label='–ü—Ä–æ–≤–µ—Ä–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax2.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax2.set_ylabel('–ü–æ—Ç–µ—Ä–∏')\n",
        "ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏ –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n–ó–∞–¥–∞–Ω–∏–µ 1 –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 2 - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–∏—Ñ—Ä MNIST —Å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "\n",
        "–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MNIST —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
        "\n",
        "- batch_size = 64\n",
        "- epochs = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö MNIST:\")\n",
        "print(f\"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {train_images.shape}\")\n",
        "print(f\"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {test_images.shape}\")\n",
        "\n",
        "# –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(train_images[i], cmap='gray')\n",
        "    ax.set_title(f'–ú–µ—Ç–∫–∞: {train_labels[i]}')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('–ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ MNIST')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:\")\n",
        "model.summary()\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "print(\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: batch_size=64, epochs=20\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=20,           # –ò–∑–º–µ–Ω–µ–Ω–æ —Å 5 –Ω–∞ 20\n",
        "    batch_size=64,       # –ò–∑–º–µ–Ω–µ–Ω–æ —Å 128 –Ω–∞ 64\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "print(\"\\n–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"–ü–æ—Ç–µ—Ä–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_loss:.4f}\")\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏–∑ —É—á–µ–±–Ω–∏–∫–∞ (–ø—Ä–∏–º–µ—Ä 2)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
        "print(\"=\"*60)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä 2 (batch_size=128, epochs=5): ~97.88%\")\n",
        "print(f\"–ù–∞—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç (batch_size=64, epochs=20): {test_acc*100:.2f}%\")\n",
        "if test_acc > 0.9788:\n",
        "    print(\"‚úì –¢–æ—á–Ω–æ—Å—Ç—å –£–õ–£–ß–®–ò–õ–ê–°–¨ –∑–∞ —Å—á—ë—Ç –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö!\")\n",
        "else:\n",
        "    print(\"‚ö† –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∞—è –∂–µ –∏–ª–∏ —á—É—Ç—å –Ω–∏–∂–µ\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax1.plot(history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax1.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')\n",
        "ax1.set_title('–¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax2.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞')\n",
        "ax2.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax2.set_ylabel('–ü–æ—Ç–µ—Ä–∏')\n",
        "ax2.set_title('–ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n–ó–∞–¥–∞–Ω–∏–µ 2 –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –ó–∞–¥–∞–Ω–∏–µ 3 - –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –Ω–∞ MNIST\n",
        "\n",
        "–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (CNN) —Å 5 —Å–ª–æ—è–º–∏ Conv2D –∏ 4 —Å–ª–æ—è–º–∏ MaxPooling2D –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π MNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CNN (–¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª)\n",
        "train_images_cnn = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images_cnn = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\")\n",
        "print(\"=\"*60)\n",
        "print(\"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 5 —Å–ª–æ–µ–≤ Conv2D –∏ 4 —Å–ª–æ—è MaxPooling2D\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ CNN —Å 5 Conv2D –∏ 4 MaxPooling2D —Å–ª–æ—è–º–∏\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "# –ü–µ—Ä–≤—ã–π –±–ª–æ–∫: Conv2D -> MaxPooling2D\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# –í—Ç–æ—Ä–æ–π –±–ª–æ–∫: Conv2D -> MaxPooling2D  \n",
        "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫: Conv2D -> MaxPooling2D\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –∏ –ø—è—Ç—ã–π —Å–ª–æ–∏ Conv2D (–±–µ–∑ MaxPooling –ø–æ—Å–ª–µ –Ω–∏—Ö, —Ç.–∫. —Ä–∞–∑–º–µ—Ä —É–∂–µ –º–∞–ª)\n",
        "x = layers.Conv2D(128, (2, 2), activation='relu')(x)\n",
        "\n",
        "# Flatten –∏ Dense —Å–ª–æ–∏\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "cnn_model = keras.Model(inputs=inputs, outputs=outputs, name=\"cnn_mnist_5conv\")\n",
        "\n",
        "# –ö–æ–º–ø–∏–ª—è—Ü–∏—è\n",
        "cnn_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# –í—ã–≤–æ–¥ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
        "print(\"\\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏:\")\n",
        "cnn_model.summary()\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "print(\"\\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_images_cnn, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"\\n–û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(test_images_cnn, test_labels, verbose=0)\n",
        "print(f\"–¢–æ—á–Ω–æ—Å—Ç—å CNN: {test_acc_cnn:.4f} ({test_acc_cnn*100:.2f}%)\")\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∏–º–µ—Ä–æ–º 3 –∏–∑ —É—á–µ–±–Ω–∏–∫–∞\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
        "print(\"=\"*60)\n",
        "print(\"–ü—Ä–∏–º–µ—Ä 3 (–ø—Ä–æ—Å—Ç–∞—è CNN): ~99.1%\")\n",
        "print(f\"–ù–∞—à–∞ CNN (5 Conv2D, 4 MaxPooling2D): {test_acc_cnn*100:.2f}%\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history_cnn.history['accuracy'], label='–û–±—É—á–∞—é—â–∞—è')\n",
        "ax1.plot(history_cnn.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è')\n",
        "ax1.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax1.set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')\n",
        "ax1.set_title('CNN: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history_cnn.history['loss'], label='–û–±—É—á–∞—é—â–∞—è')\n",
        "ax2.plot(history_cnn.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è')\n",
        "ax2.set_xlabel('–≠–ø–æ—Ö–∞')\n",
        "ax2.set_ylabel('–ü–æ—Ç–µ—Ä–∏')\n",
        "ax2.set_title('CNN: –ü–æ—Ç–µ—Ä–∏ –ø–æ —ç–ø–æ—Ö–∞–º')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö\n",
        "print(\"\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:\")\n",
        "predictions = cnn_model.predict(test_images_cnn[:10])\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(test_images_cnn[i].reshape(28, 28), cmap='gray')\n",
        "    pred_label = np.argmax(predictions[i])\n",
        "    true_label = test_labels[i]\n",
        "    color = 'green' if pred_label == true_label else 'red'\n",
        "    ax.set_title(f'–ü—Ä–µ–¥—Å: {pred_label}\\n–ò—Å—Ç–∏–Ω: {true_label}', color=color)\n",
        "    ax.axis('off')\n",
        "plt.suptitle('–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π CNN')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n–ó–∞–¥–∞–Ω–∏–µ 3 –≤—ã–ø–æ–ª–Ω–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
        "\n",
        "–†–∞–±–æ—á–∞—è —Ç–µ—Ç—Ä–∞–¥—å 7 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!\n",
        "\n",
        "**–ß—Ç–æ –±—ã–ª–æ –∏–∑—É—á–µ–Ω–æ:**\n",
        "\n",
        "1. –û—Å–Ω–æ–≤—ã –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ –∏ –µ–≥–æ –æ–±—É—á–µ–Ω–∏–µ\n",
        "2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (sigmoid, ReLU, tanh)\n",
        "3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Scikit-Learn –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (MLPClassifier –∏ MLPRegressor)\n",
        "4. –†–∞–±–æ—Ç–∞ —Å TensorFlow –∏ Keras\n",
        "5. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π MNIST\n",
        "6. –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN)\n",
        "\n",
        "**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:**\n",
        "\n",
        "- –ü–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–∏–ª—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ\n",
        "- MLPClassifier –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Iris\n",
        "- MLPRegressor —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É\n",
        "- CNN –¥–æ—Å—Ç–∏–≥–ª–∞ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (>99%) –Ω–∞ MNIST\n",
        "\n",
        "–í—Å–µ –∑–∞–¥–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã! üéâ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
